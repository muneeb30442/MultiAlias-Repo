{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7d39bc2-3b90-473b-952f-4e5dd92d61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "import numpy\n",
    "import matplotlib\n",
    "import pandas\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d98ddd9f-04c1-4a09-8095-7521578cee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c1a45c-4a9a-447b-8950-e91a7062727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference no.1 = https://blog.finxter.com/5-best-ways-to-remove-black-background-and-make-it-transparent-using-opencv-python/\n",
    "# Reference no.2 = https://www.geeksforgeeks.org/image-thresholding-in-python-opencv/\n",
    "# Reference no.3 = https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html\n",
    "# Reference no.4 = https://herts.instructure.com/courses/112101/pages/running-python-simply?module_item_id=36661346\n",
    "# Reference no.5 = https://machinelearningmastery.com/machine-learning-in-python-step-by-step/\n",
    "# Reference no.6 = https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "# Reference no.7 = https://www.geeksforgeeks.org/python-extracting-rows-using-pandas-iloc/\n",
    "# Reference no.8 = https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html\n",
    "# Reference no.9 = https://stackoverflow.com/questions/37891954/keras-how-do-i-predict-after-i-trained-a-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c44e4dce-6cd0-41b2-adf9-f1f5230f4fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All normal files processed.\n"
     ]
    }
   ],
   "source": [
    "# The directory path where the healthy images are located\n",
    "directory = \"C:/images/NORMAL\" \n",
    "\n",
    "# Get all the image files in the directory\n",
    "image_files = [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.jfif' ))]\n",
    "\n",
    "# Process each image file\n",
    "for image_file in image_files:\n",
    "    # Read the image\n",
    "    image_path = os.path.join(directory, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to greyscale\n",
    "    grey_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply CLAHE (Contrast-Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))  # Set CLAHE parameters\n",
    "    equalised_image = clahe.apply(grey_image)  # Apply CLAHE\n",
    "\n",
    "    # Apply Otsu's thresholding to enhance lung regions\n",
    "    _, thresholded = cv2.threshold(equalised_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:  # Ensure at least one contour is found\n",
    "        # Assume the largest external contour is the object to keep\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        # Crop the image using the dimensions of the bounding rectangle\n",
    "        crop = equalised_image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Resize image to 224x224 for deep learning models\n",
    "        resized_crop = cv2.resize(crop, (224, 224))\n",
    "\n",
    "        # Normalize pixel values to range [0,1]\n",
    "        normalized_image = resized_crop / 255.0\n",
    "\n",
    "        # Create new alpha channel with same dimensions as cropped image\n",
    "        alpha_channel = numpy.ones(resized_crop.shape[:2], dtype='uint8') * 255\n",
    "        \n",
    "        # Add alpha channel to cropped image\n",
    "        rgba = cv2.merge((*cv2.split(resized_crop), alpha_channel))\n",
    "\n",
    "        # Create the CSV file name by removing the image extension and adding .csv\n",
    "        csv_file_name = os.path.splitext(image_file)[0] + '.csv'\n",
    "        csv_file_path = os.path.join(directory, csv_file_name)\n",
    "\n",
    "        # Open the CSV file and write the headers and greyscale values\n",
    "        with open(csv_file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Greyscale\"])\n",
    "\n",
    "            # Write the greyscale values for each pixel\n",
    "            for i in range(resized_crop.shape[0]):\n",
    "                for j in range(resized_crop.shape[1]):\n",
    "                    grey_value = resized_crop[i, j]\n",
    "                    writer.writerow([grey_value])\n",
    "\n",
    "print(\"All normal files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22178a3-80fd-41ce-9cd8-3bfa8f21fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the CSV files\n",
    "summary_file_path = os.path.join(directory, \"normal summary means.csv\")\n",
    "\n",
    "# Initialise a list to store the summary data\n",
    "summary_data = []\n",
    "\n",
    "# Process each CSV file in the directory\n",
    "for csv_file in os.listdir(directory):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        csv_file_path = os.path.join(directory, csv_file)\n",
    "        with open(csv_file_path, 'r', newline='') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip header\n",
    "\n",
    "            # Initialise the sum and count variables\n",
    "            grey_sum = 0\n",
    "            count = 0\n",
    "\n",
    "            # Sum greyscale values\n",
    "            for row in reader:\n",
    "                if row:  # Check if the row is not empty\n",
    "                    grey_value = float(row[0])  # There's only one greyscale value per row\n",
    "                    grey_sum += grey_value\n",
    "                    count += 1\n",
    "\n",
    "            # Calculate the mean greyscale value\n",
    "            grey_mean = grey_sum / count if count else 0\n",
    "\n",
    "            # Add the calculated mean to the summary list with the file name as \"Building\" column\n",
    "            summary_data.append([grey_mean, \"Normal\", os.path.splitext(csv_file)[0]])\n",
    "\n",
    "# Write the summary data to a new CSV file with the appropriate headers\n",
    "with open(summary_file_path, 'w', newline='') as summary_file:\n",
    "    writer = csv.writer(summary_file)\n",
    "    writer.writerow([\"Greyscale Mean\", \"Category\", \"File Name\"])\n",
    "    for data in summary_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(\"Summary of means for normal has been written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeaaa46-1e3d-456c-85e5-67b19a655b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory path where the healthy images are located\n",
    "directory = \"C:/images/PNEUMONIA\" \n",
    "\n",
    "# Get all the image files in the directory\n",
    "image_files = [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.jfif' ))]\n",
    "\n",
    "# Process each image file\n",
    "for image_file in image_files:\n",
    "    # Read the image\n",
    "    image_path = os.path.join(directory, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to greyscale\n",
    "    grey_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply CLAHE (Contrast-Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))  # Set CLAHE parameters\n",
    "    equalised_image = clahe.apply(grey_image)  # Apply CLAHE\n",
    "\n",
    "    # Apply Otsu's thresholding to enhance lung regions\n",
    "    _, thresholded = cv2.threshold(equalised_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:  # Ensure at least one contour is found\n",
    "        # Assume the largest external contour is the object to keep\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        # Crop the image using the dimensions of the bounding rectangle\n",
    "        crop = equalised_image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Resize image to 224x224 for deep learning models\n",
    "        resized_crop = cv2.resize(crop, (224, 224))\n",
    "\n",
    "        # Normalize pixel values to range [0,1]\n",
    "        normalized_image = resized_crop / 255.0\n",
    "\n",
    "        # Create new alpha channel with same dimensions as cropped image\n",
    "        alpha_channel = numpy.ones(resized_crop.shape[:2], dtype='uint8') * 255\n",
    "        \n",
    "        # Add alpha channel to cropped image\n",
    "        rgba = cv2.merge((*cv2.split(resized_crop), alpha_channel))\n",
    "\n",
    "        # Create the CSV file name by removing the image extension and adding .csv\n",
    "        csv_file_name = os.path.splitext(image_file)[0] + '.csv'\n",
    "        csv_file_path = os.path.join(directory, csv_file_name)\n",
    "\n",
    "        # Open the CSV file and write the headers and greyscale values\n",
    "        with open(csv_file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Greyscale\"])\n",
    "\n",
    "            # Write the greyscale values for each pixel\n",
    "            for i in range(resized_crop.shape[0]):\n",
    "                for j in range(resized_crop.shape[1]):\n",
    "                    grey_value = resized_crop[i, j]\n",
    "                    writer.writerow([grey_value])\n",
    "\n",
    "print(\"All pneumonia files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667fd02c-8147-4428-9e2d-ec6757d2a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the CSV files\n",
    "summary_file_path = os.path.join(directory, \"pneumonia summary means.csv\")\n",
    "\n",
    "# Initialise a list to store the summary data\n",
    "summary_data = []\n",
    "\n",
    "# Process each CSV file in the directory\n",
    "for csv_file in os.listdir(directory):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        csv_file_path = os.path.join(directory, csv_file)\n",
    "        with open(csv_file_path, 'r', newline='') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip header\n",
    "\n",
    "            # Initialise the sum and count variables\n",
    "            grey_sum = 0\n",
    "            count = 0\n",
    "\n",
    "            # Sum greyscale values\n",
    "            for row in reader:\n",
    "                if row:  # Check if the row is not empty\n",
    "                    grey_value = float(row[0])  # There's only one greyscale value per row\n",
    "                    grey_sum += grey_value\n",
    "                    count += 1\n",
    "\n",
    "            # Calculate the mean greyscale value\n",
    "            grey_mean = grey_sum / count if count else 0\n",
    "\n",
    "            # Add the calculated mean to the summary list with the file name as \"Building\" column\n",
    "            summary_data.append([grey_mean, \"Pneumonia\", os.path.splitext(csv_file)[0]])\n",
    "\n",
    "# Write the summary data to a new CSV file with the appropriate headers\n",
    "with open(summary_file_path, 'w', newline='') as summary_file:\n",
    "    writer = csv.writer(summary_file)\n",
    "    writer.writerow([\"Greyscale Mean\", \"Category\", \"File Name\"])\n",
    "    for data in summary_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(\"Summary of means for pneumonia has been written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80edf1-59bd-41aa-ad3a-4f56fe9661da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory path where the healthy images are located\n",
    "directory = \"C:/images/TEST SET\" \n",
    "\n",
    "# Get all the image files in the directory\n",
    "image_files = [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.jfif' ))]\n",
    "\n",
    "# Process each image file\n",
    "for image_file in image_files:\n",
    "    # Read the image\n",
    "    image_path = os.path.join(directory, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to greyscale\n",
    "    grey_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply CLAHE (Contrast-Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))  # Set CLAHE parameters\n",
    "    equalised_image = clahe.apply(grey_image)  # Apply CLAHE\n",
    "\n",
    "    # Apply Otsu's thresholding to enhance lung regions\n",
    "    _, thresholded = cv2.threshold(equalised_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:  # Ensure at least one contour is found\n",
    "        # Assume the largest external contour is the object to keep\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        # Crop the image using the dimensions of the bounding rectangle\n",
    "        crop = equalised_image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Resize image to 224x224 for deep learning models\n",
    "        resized_crop = cv2.resize(crop, (224, 224))\n",
    "\n",
    "        # Normalize pixel values to range [0,1]\n",
    "        normalized_image = resized_crop / 255.0\n",
    "\n",
    "        # Create new alpha channel with same dimensions as cropped image\n",
    "        alpha_channel = numpy.ones(resized_crop.shape[:2], dtype='uint8') * 255\n",
    "        \n",
    "        # Add alpha channel to cropped image\n",
    "        rgba = cv2.merge((*cv2.split(resized_crop), alpha_channel))\n",
    "\n",
    "        # Create the CSV file name by removing the image extension and adding .csv\n",
    "        csv_file_name = os.path.splitext(image_file)[0] + '.csv'\n",
    "        csv_file_path = os.path.join(directory, csv_file_name)\n",
    "\n",
    "        # Open the CSV file and write the headers and greyscale values\n",
    "        with open(csv_file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Greyscale\"])\n",
    "\n",
    "            # Write the greyscale values for each pixel\n",
    "            for i in range(resized_crop.shape[0]):\n",
    "                for j in range(resized_crop.shape[1]):\n",
    "                    grey_value = resized_crop[i, j]\n",
    "                    writer.writerow([grey_value])\n",
    "\n",
    "print(\"All test files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a638d6-da27-48ab-b3f1-2c4e73a228bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the CSV files\n",
    "summary_file_path = os.path.join(directory, \"test summary means.csv\")\n",
    "\n",
    "# Initialise a list to store the summary data\n",
    "summary_data = []\n",
    "\n",
    "# Process each CSV file in the directory\n",
    "for csv_file in os.listdir(directory):\n",
    "    if csv_file.endswith('.csv'):\n",
    "        csv_file_path = os.path.join(directory, csv_file)\n",
    "        with open(csv_file_path, 'r', newline='') as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip header\n",
    "\n",
    "            # Initialise the sum and count variables\n",
    "            grey_sum = 0\n",
    "            count = 0\n",
    "\n",
    "            # Sum greyscale values\n",
    "            for row in reader:\n",
    "                if row:  # Check if the row is not empty\n",
    "                    grey_value = float(row[0])  # There's only one greyscale value per row\n",
    "                    grey_sum += grey_value\n",
    "                    count += 1\n",
    "\n",
    "            # Calculate the mean greyscale value\n",
    "            grey_mean = grey_sum / count if count else 0\n",
    "\n",
    "            # Add the calculated mean to the summary list with the file name as \"Building\" column\n",
    "            summary_data.append([grey_mean, \"Test\", os.path.splitext(csv_file)[0]])\n",
    "\n",
    "# Write the summary data to a new CSV file with the appropriate headers\n",
    "with open(summary_file_path, 'w', newline='') as summary_file:\n",
    "    writer = csv.writer(summary_file)\n",
    "    writer.writerow([\"Greyscale Mean\", \"Category\", \"File Name\"])\n",
    "    for data in summary_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(\"Summary of means for test has been written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614e907-053b-4791-9674-3ae1664d4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of summary CSV files\n",
    "summary_files = [\n",
    "    \"C:/images/Normal/normal summary means.csv\",\n",
    "    \"C:/images/Pneumonia/pneumonia summary means.csv\"\n",
    "]\n",
    "# Directory to store the training CSV file\n",
    "combined_summary_file_path = \"C:/images/combined summary means.csv\"\n",
    "\n",
    "# Write the 2D array (list of lists) to a CSV file.\n",
    "with open(combined_summary_file_path, 'w', newline='') as combined_file:\n",
    "    writer = csv.writer(combined_file)\n",
    "\n",
    "    # Process each summary CSV file\n",
    "    for i, summary_file in enumerate(summary_files):\n",
    "        with open(summary_file, 'r', newline='') as file:\n",
    "            reader = csv.reader(file)\n",
    "            if i == 0:  # If it's the first file, write the header\n",
    "                writer.writerow(next(reader))\n",
    "            else:\n",
    "                next(reader)  # Skip the header for the rest of the files\n",
    "            writer.writerows(reader)  # Write the data\n",
    "\n",
    "print(\"Combined summary has been written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63599554-ff5a-41d0-a313-03868c2c56bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise the data\n",
    "url = \"C:/images/combined summary means.csv\"\n",
    "names = ['Greyscale Mean', 'category']  # Adjusting for greyscale\n",
    "dataset = pandas.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b17fc-7040-4322-96a1-d0250c62a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b2234-2ace-45bd-a3cb-6f6cf90e8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head\n",
    "print(dataset.head(5803))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf7f819-e57a-4ce1-9896-a7c7e759052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptions\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc5b2c-4f99-4613-96b3-dec2f1a97791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "print(dataset.groupby('Category').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997ea28-f460-4edf-8113-631c6c131efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box and whisker plots - adjusted for single feature\n",
    "dataset.plot(kind='box', y='Greyscale Mean', title='Box Plot of Greyscale Means')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c71187-b2be-421c-9c0e-be2d2d794798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms - adjusted for single feature\n",
    "dataset['Greyscale Mean'].hist()\n",
    "plt.title('Histogram of Greyscale Means')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ae814-dfe9-4f8c-a215-f309b903229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out validation dataset THIS IS AN IMPORTANT STEP\n",
    "array = dataset.values\n",
    "X = array[:,0:1]  # Adjusted to select only the greyscale column\n",
    "y = array[:,1]\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.2, random_state=1) #THE .20 REFERS TO THE PERCENTAGE OF THE FILES WHICH WILL BE VALIDATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48458f1-cd03-4dd3-ac16-9284cf0918d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))  # BEST FIT MODEL FOR BINARY DISTRIBUTION\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))  # BUILDS USING ON VALUES TO FIND THE N VALUE BETWEEN CLUSTERS\n",
    "models.append(('CART', DecisionTreeClassifier())) # BEST DECISION IN DECISON TREE SELECTS THE BEST\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "\tkfold = StratifiedKFold(n_splits=3, random_state=1, shuffle=True)\n",
    "\tcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tprint('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824603ce-c334-4430-b0a4-12cb79630262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "plt.boxplot(results, labels=names)\n",
    "plt.title('Algorithm Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51099d06-1b65-42d9-b099-bda504cad6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(gamma='auto')\n",
    "model.fit(X_train, Y_train)\n",
    "predictions = model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e44ebc-3f21-4c69-9c66-bff6775674d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d610ed0-3d5d-44b0-b9eb-5d685f3c0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = ['C:/images/TEST SET/test summary means.csv']\n",
    "\n",
    "# Load each CSV file and concatenate into one DataFrame [6]\n",
    "unseen_data_frames = [pandas.read_csv(f) for f in directory]\n",
    "data = pandas.concat(unseen_data_frames, ignore_index=True)\n",
    "\n",
    "# Select test criteria [7][8]\n",
    "features_for_prediction = data.iloc[:, [0]].values\n",
    "\n",
    "# Select the trained model [9]\n",
    "predictions = model.predict(features_for_prediction)\n",
    "\n",
    "# Print predictions for each unseen image\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f\"Unseen image {i} predicted as: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b5471-e088-4335-b464-dee581ab23f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
